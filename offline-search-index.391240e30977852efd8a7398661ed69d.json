[{"body":"Pre-built binaries, as well as Debian and RPM packages, are available here for Linux, specifically x86_64 and arm64 architectures.\nTo start a container hosting wfx, follow these commands:\n# create a named volume to persist data (only needed the first time) docker volume create wfx-db docker run --rm -v wfx-db:/home/nonroot \\ -p 8080:8080 -p 8081:8081 \\ ghcr.io/siemens/wfx:latest If pre-built binaries are not available (refer to go tool dist list for alternative platforms and architectures, such as Windows or macOS), or if specific features need to be disabled during compilation, building wfx from source is necessary.\nBuilding wfx A recent Go compiler (see go.mod) as well as GNU make wrapping the go build commands is required to build wfx and its associated tools:\nmake The above command produces the following binaries:\nwfx: The server component providing the RESTful APIs for managing workflows and jobs. wfxctl: Command line client for interacting with the wfx. wfx-loadtest: Command line tool for load-testing a wfx instance. wfx-viewer: Convenience tool to visualize workflows in different formats (e.g. PlantUML, Mermaid). All binaries have extensive help texts when invoked with --help.\nBuild Tags Go build tags are used to select compiled-in support for various features. The following persistent storage selection build tags are available:\nBuild Tag Description sqlite Enable built-in SQLite support libsqlite3 Dynamically link against libsqlite3 postgres Enable built-in PostgreSQL support mysql Enable built-in MySQL support plugin Enable support for external plugins By default, all built-in persistent storage options are enabled (wfx requires at least one persistent storage to save workflows and jobs).\nNote that the selection of build tags can impact the size of the wfx binary file and may as well have implications for the software clearing process, including obligations that must be met.\nTo build and compile-in, e.g., SQLite persistent storage support only, according GO_TAGS must be given:\nmake GO_TAGS=sqlite Debian The Go toolchain provided by Debian stable is often outdated; it’s typically end-of-life upstream but still maintained by Debian’s security team. Therefore, to compile wfx from source in Debian stable, the -backports repository is necessary. In contrast, for Debian testing, it usually works out of the box since it ships with a recent version of the Go toolchain.\nInstalling wfx wfx’s release binaries are statically linked and self-contained. Hence, an installation isn’t strictly necessary, although if available, it’s recommended to pick the distro packages (e.g. *.deb for Debian-based distros).\nNevertheless, for convenience on UNIXy systems,\nmake DESTDIR= prefix= install installs the binaries to /bin. Giving a different DESTDIR and/or prefix allows to adjust to other locations.\nAlternatively, a pre-built Debian package is provided.\nFor convenience and ease of use, all binaries come with shell completions available for Bash, Fish and Zsh. To install the completions, refer to the binary’s completion --help output, e.g. wfx completion bash --help.\n","categories":"","description":"","excerpt":"Pre-built binaries, as well as Debian and RPM packages, are available …","ref":"/docs/installation/","tags":"","title":"Build and Installation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/categories/","tags":"","title":"Categories"},{"body":"wfx is configured in the following order of precedence using\ncommand line parameters (e.g. --log-level), environment variables (prefixed with WFX_, e.g. WFX_LOG_LEVEL), configuration files in YAML format (either via the --config command line parameter or present in one of the default search locations, see wfx --help). Note that wfx supports configuration file live reloading so that a running wfx instance can be reconfigured without the need for restarting it.\nWithout configuration,\n/usr/bin/wfx starts wfx using the built-in SQLite-based persistent storage. The northbound (management) API is available at [[ http://127.0.0.1:8081/api/wfx/v1/\\(http://127.0.0.1:8081/api/wfx/v1/)](http://127.0.0.1:8081/api/wfx/v1/](http://127.0.0.1:8081/api/wfx/v1/)), whereas the southbound (client) API is available at a different port: [[ http://127.0.0.1:8080/api/wfx/v1\\(http://127.0.0.1:8080/api/wfx/v1)](http://127.0.0.1:8080/api/wfx/v1](http://127.0.0.1:8080/api/wfx/v1)).\nSystemd Integration For production deployments, it’s recommended to run wfx under a service supervisor such as systemd. The share/systemd directory provides pre-configured systemd service units. These units are also included in the distribution packages available with wfx releases.\nsystemctl enable --now wfx@foo.socket # multiple instances of wfx can be running at the same time, e.g. systemctl enable --now wfx@bar.socket The wfx services launch on-demand, i.e., they start when a client connects, such as when retrieving the wfx version:\nwfxctl --client-unix-socket /var/run/wfx/foo/client.sock version Persistent Storage wfx requires a persistent storage to save workflows and jobs. The default persistent storage is SQLite and requires no further configuration.\nThe command line argument --storage={sqlite,postgres,mysql} is available to choose a persistent storage backend out of the compiled-in available ones at run-time. Each persistent storage allows further individual configuration via --storage-opt=\u003coptions\u003e.\nNote that wfx needs to manage the database schema and hence needs appropriate permissions to, e.g., create tables. This is in particular important for the PostgreSQL and MySQL persistent storage options as they’re connecting to an external database service which has to be setup accordingly.\nSQLite SQLite is the default persistent storage and automatically selected if no other persistent storage option is given. It can be further configured with the --storage-opt configuration option, see the go-sqlite3 Wiki for available Data Source Name (DSN) options.\nAs an example, the following command runs a wfx instance with an ephemeral in-memory SQLite database:\nwfx --storage sqlite --storage-opt \"file:wfx?mode=memory\u0026cache=shared\u0026_fk=1\" Note that all state is lost on wfx exiting so it’s advised to use it for testing purposes only.\nPostgreSQL PostgreSQL is a well-known open source object-relational database. Via environment variables or a Data Source Name (DSN) passed as --storage-opt, the link to a PostgreSQL instance is configured, see PostgreSQL’s parameter key word names for DSN and environment variables documentation for details and available options.\nAs an example, the following two commands each run a wfx instance connecting to the same PostgreSQL instance but with different configuration means:\n# Configuration via DSN key=value string wfx --storage postgres \\ --storage-opt \"host=localhost port=5432 user=wfx password=secret database=wfx\" \u0026 # Configuration using environment variables env PGHOST=localhost \\ PGPORT=5432 \\ PGUSER=wfx \\ PGPASSWORD=secret \\ PGDATABASE=wfx \\ wfx --storage postgres MySQL MySQL is another well-known open source relational database.\nNote that MariaDB is currently unsupported due to the lack of certain JSON features (specifically the inability to directly index JSON data).\nWith the Data Source Name (DSN) passed as --storage-opt, the link to a MySQL instance is configured, see Go’s SQL Driver for available options and as reference.\nAs an example, the following command runs a wfx instance connecting to a MySQL instance using similar configuration options as for PostgreSQL:\n# Configuration via DSN URL string wfx --storage mysql \\ --storage-opt \"wfx:secret@tcp(localhost:3306)/wfx\" Communication Channels wfx currently supports the following network communication channels:\nhttp: Unencrypted HTTP https: HTTP over TLS (Transport Layer Security) unix: Unix-domain sockets With the --scheme configuration option, one or multiple from the preceding list are enabled. For instance, to use wfx in HTTPS-only mode:\nwfx --scheme=https \\ --tls-certificate=localhost/cert.pem \\ --tls-key=localhost/key.pem To enable both, HTTP and HTTPS, simultaneously, bound to different hosts:\nwfx --scheme=http,https \\ --client-host=localhost \\ --client-tls-host=0.0.0.0 \\ --tls-certificate=localhost/cert.pem \\ --tls-key=localhost/key.pem To exclusively use Unix-domain sockets:\nwfx --scheme unix \\ --client-unix-socket /tmp/wfx-client.sock \\ --mgmt-unix-socket /tmp/wfx-mgmt.sock The following connectivity parameters are available:\nParameter Description --scheme One or multiple communication schemes to be used for client-server communication. --client-host The address to listen on for client HTTP requests --client-port The port to listen on for client HTTP requests --client-tls-host Same as --client-host but for HTTP over TLS --client-tls-port Same as --client-port but for HTTP over TLS --mgmt-host The address to listen on for wfx management / operator HTTP requests --mgmt-port The port to listen on for wfx management /operator HTTP requests --mgmt-tls-host Same as --mgmt-host but for HTTP over TLS --mgmt-tls-port Same as --mgmt-port but for HTTP over TLS --tls-certificate The location of the TLS certificate file --tls-key The location of the TLS key file --tls-ca The certificate authority certificate file for mutual TLS authentication File Server wfx comes with a built-in file server that serves artifacts at http://\u003cwfx host:{client,mgmt} port\u003e/download/. This feature is particularly useful for dynamic deployments or when an external file storage solution is unavailable. To configure the directory that backs the file server URL’s contents, use the --simple-fileserver=/path/to/folder option.\nNote that this feature is disabled by default and must be explicitly enabled at run-time using this option.\n","categories":"","description":"","excerpt":"wfx is configured in the following order of precedence using\ncommand …","ref":"/docs/configuration/","tags":"","title":"Configuration"},{"body":"The Device Artifact Update Workflow Family has been specifically designed for updating software on devices.\nThe following clients have support for the Device Artifact Update Workflow Family:\nSWUpdate - Software Update for Embedded Linux Devices. Workflows Currently, the DAU Workflow Family comprises two workflows that model the software update process for devices:\nThe wfx.workflow.dau.direct workflow caters for the fully automated software update use case while the wfx.workflow.dau.phased workflow operates in distinct phases requiring external input to advance the workflow.\nInstallation wfx does not come with any pre-loaded or pre-seeded workflows, thus requiring the creation of workflows either locally\nwfxctl workflow create workflow/dau/*.yml or remotely, e.g. using curl:\ncurl -Ls https://github.com/siemens/wfx/raw/main/workflow/dau/wfx.workflow.dau.phased.yml | wfxctl workflow create - curl -Ls https://github.com/siemens/wfx/raw/main/workflow/dau/wfx.workflow.dau.direct.yml | wfxctl workflow create - Modus Operandi Both workflows operate in stages, each of which follows the Command, Feedback, and Completion (CFC) scheme. These stages consist of:\na command state which instructs the device to start the update process, an action + feedback state during which the device performs the necessary work and provides progress updates to wfx, and a completion state in which the device signals the completion of the stage. A sequence of such CFC loops constitutes the particular workflow. In case of wfx.workflow.dau.direct, the wfx transitions to the next stage automatically while in case of wfx.workflow.dau.phased, wfx waits for external input, e.g., from an operator.\nwfx.workflow.dau.direct The wfx.workflow.dau.direct workflow consists of the two stages installation and activation: During the installation stage, the device downloads and installs the update artifacts. In the subsequent activation stage, the device takes action to activate the update. Depending on the type of artifact(s), this activation action varies as, e.g., container images need a different activation action than firmware disk images (see below Section Job Definition).\nThe graph representation of the wfx.workflow.dau.direct workflow is depicted in the following figure, omitting state descriptions and transition eligibles for legibility:\nINSTALL ────────┐ │ │ ├─◀─┐ │ ▼ │ │ INSTALLING ──────┤ │ │ ▼ │ INSTALLED │ │ │ ▼ │ ACTIVATE ───────┤ │ │ ├─◀─┐ │ ▼ │ │ ACTIVATING ──────┤ │ │ ▼ ▼ ACTIVATED TERMINATED wfx.workflow.dau.phased The wfx.workflow.dau.phased workflow is similar to the wfx.workflow.dau.direct workflow but starts in the CREATED state instead and introduces another download stage in between installation and activation to decouple artifact download from its installation: The initial state CREATED serves as an anchor to actually kickstart the external input-driven CFC scheme. With the additional download stage, a maintenance window could be realized prior to which the artifact is downloaded but only installed when wfx commands the begin of the installation phase (e.g. a certain time window has been reached).\nConsequently ― and in contrast to the wfx.workflow.dau.direct ― wfx doesn’t transition to the next stage automatically but waits for external input to do so, e.g., by an operator.\nThe graph representation of the wfx.workflow.dau.phased workflow is depicted in the following figure, again omitting state descriptions and transition eligibles for legibility:\nCREATED │ ▼ DOWNLOAD ───────┐ │ │ ├─◀─┐ │ ▼ │ │ DOWNLOADING ──────┤ │ │ ▼ │ DOWNLOADED │ │ │ ▼ │ INSTALL ────────┤ │ │ ├─◀─┐ │ ▼ │ │ INSTALLING ──────┤ │ │ ▼ │ INSTALLED │ │ │ ▼ │ ACTIVATE ───────┤ │ │ ├─◀─┐ │ ▼ │ │ ACTIVATING ──────┤ │ │ ▼ ▼ ACTIVATED TERMINATED Job Definition As being general purpose, wfx doesn’t impose a particular schema on the information conveyed to the device describing its action(s) to perform, except that it’s in JSON format. Instead, the job definition is a contract between the operator creating jobs, each possibly following a different workflow, and the client(s) executing those jobs in lock-step with the wfx. The same is true for the type of update artifacts that are specified in the job definition and that can be of any form such as, e.g., firmware disk images, container images, or configurations: The operator has to has to take care to only assign jobs to devices that are known to be able to digest this type of update artifact. wfx doesn’t exercise any checks on compatibility.\nAn exemplary job for a device called example (utilizing the built-in simple file server) may be created as follows:\ncat \u003c\u003cEOF | wfxctl job create --client-id example --workflow wfx.workflow.dau.direct - { \"version\": \"1.0\", \"type\": [\"firmware\"], \"artifacts\": [ { \"name\": \"Example Device Firmware Artifact\", \"version\": \"1.1\", \"uri\": \"http://wfx.host:8080/download/example_artifact.swu\" } ] } EOF The type list field allows to label update jobs. Labels may be used by wfx’s on-device counterpart to determine the activation action(s) to execute since, e.g., container images need a different activation action than firmware disk images or a configuration change.\nIn the preceding example, the presence of the firmware label may be used to instruct the on-device client to test-reboot into the new firmware.\nSince wfx isn’t concerned with the job definition except for conveying it to the device, it can be adapted to specific needs by feeding in a different job definition into the wfx on job creation and having a client on the device that can digest it.\n","categories":"","description":"","excerpt":"The Device Artifact Update Workflow Family has been specifically …","ref":"/docs/workflows/dau/","tags":"","title":"Device Artifact Update (DAU)"},{"body":"","categories":"","description":"","excerpt":"","ref":"/docs/","tags":"","title":"Documentation"},{"body":"API wfx provides two RESTful APIs to interact with it: the northbound operator/management interface and the southbound interface used by clients as illustrated in the following figure:\nManagement │ │ ▼ Northbound API ┌──────────────────┐ │ wfx │ └──────────────────┘ ▲ Southbound API │ │ Device The northbound API is used to create jobs and execute server-side state transitions, whereas the southbound API is used for client-side transitions.\nThe complete wfx API specification is accessible at runtime via the /swagger.json endpoint. Clients may inspect this specification at run-time so to obey the various limits imposed, e.g, for parameter value ranges and array lengths.\nFor convenience, wfx includes a built-in Swagger UI accessible at runtime via http://localhost:8080/api/wfx/v1/docs, assuming default listening host and port configuration.\nJob Events Job events provide a notification mechanism that informs clients about certain operations happening on jobs. This approach eliminates the need for clients to continuously poll wfx, thus optimizing network usage and client resources. Job events can be useful for user interfaces (UIs) and other applications that demand near-instantaneous updates.\nArchitecture Below is a high-level overview of how the communication flow operates:\n┌────────┐ ┌─────┐ │ Client │ │ wfx │ └────────┘ └─────┘ | | | HTTP GET /jobs/events | |-----------------------------------►| | | | | | Event Loop | ┌───|────────────────────────────────────|───┐ │ | [Content-Type: text/event-stream] | │ │ | | │ │ | | │ │ | Push Event | │ │ |◄-----------------------------------| │ │ | | │ └───|────────────────────────────────────|───┘ | | ▼ ▼ ┌────────┐ ┌─────┐ │ Client │ │ wfx │ └────────┘ └─────┘ The client initiates communication by sending an HTTP GET request to the /jobs/events endpoint. Clients may also include optional filter parameters within the request. Upon receipt of the request, wfx sets the Content-Type header to text/event-stream. The server then initiates a stream of job events in the response body, allowing clients to receive instant updates. Event Format Specification The job events stream is composed of server-sent events (SSE). Accordingly, the stream is structured as follows:\ndata: [...] id: 1 data: [...] id: 2 [...] An individual event within the stream conforms to this format:\ndata: { \"action\": \"\u003cACTION\u003e\", \"ctime\": \u003cCTIME\u003e, \"tags\": \u003cTAGS\u003e, \"job\": \u003cJOB\u003e } id: \u003cEVENT_ID\u003e\\n\\n Note: Each event is terminated by a pair of newline characters \\n\\n (as required by the SSE spec).\nThe semantics of the individual fields is:\n\u003cACTION\u003e specifies the type of event that occurred. The valid actions are: CREATE: a new job has been created DELETE: an existing job has been deleted ADD_TAGS: tags were added to a job DELETE_TAGS: tags were removed from a job UPDATE_STATUS: job status has been updated UPDATE_DEFINITION: job definition has been updated \u003cCTIME\u003e: event creation time (ISO8601) \u003cTAGS\u003e: JSON array of tags as provided by the client \u003cJOB\u003e is a JSON object containing the portion of the job object which was changed, e.g., for an UPDATE_STATUS event, the job status is sent but not its definition. To enable filtering, the fields id, clientId and workflow.name are always part of the response. \u003cEVENT_ID\u003e: an integer which uniquely identifies each event, starting at 1 and incrementing by 1 for every subsequent event. Clients can use this to identify any missed messages. If an overflow occurs, the integer resets to zero, a scenario the client can recognize and address. Example:\ndata: {\"action\":\"UPDATE_STATUS\",\"job\":{\"clientId\":\"Dana\",\"id\":\"c6698105-6386-4940-a311-de1b57e3faeb\",\"status\":{\"definitionHash\":\"adc1cfc1577119ba2a0852133340088390c1103bdf82d8102970d3e6c53ec10b\",\"state\":\"PROGRESS\"},\"workflow\":{\"name\":\"wfx.workflow.kanban\"}}} id: 1\\n\\n Filter Parameters Job events can be filtered using any combination of the following parameters:\nJob IDs Client IDs Workflow Names This enables more precise control over the dispatched events. Note that it is entirely possible to subscribe multiple times to job events using various filters in order to create a more advanced event recognition model.\nExamples wfxctl offers a reference client implementation. The following command subscribes to all job events:\nwfxctl job events This may result in a large number of events, though. For a more targeted approach, filter parameters may be used. Assuming the job IDs are known (either because the jobs have been created already or the IDs are received via another subscription channel), the following will subscribe to events matching either of the two specified job IDs:\nwfxctl job events --job-id=d305e539-1d41-4c95-b19a-2a7055c469d0 --job-id=e692ad92-45e6-4164-b3fd-8c6aa884011c See wfxctl job events --help for other filter parameters, e.g. workflow names.\nConsiderations and Limitations Asynchronous Job Status Updates: Job status updates are dispatched asynchronously to avoid the risk of a subscriber interfering with the actual job operation. In particular, there is no guarantee that the messages sent to the client arrive in a linear order (another reason for that may be networking-related). While this is typically not a concern, it could become an issue in high-concurrency situations. For example, when multiple clients try to modify the same job or when a single client issues a rapid sequence of status updates. As a result, messages could arrive in a not necessarily linear order, possibly deviating from the client’s expectation. However, the client can use the (event) id and ctime fields to establish a natural ordering of events as emitted by wfx. Unacknowledged Server-Sent Events (SSE): SSE operates on a one-way communication model and does not include an acknowledgment or handshake protocol to confirm message delivery. This design choice aligns with the fundamental principles of SSE but does mean that there’s a possibility some events may not reach the intended subscriber (which the client can possibly detect by keeping track of SSE event IDs). Event Stream Orchestration: Each wfx instance only yields the events happening on that instance. Consequently, if there are multiple wfx instances, a consolidated “global” event stream can only be assembled by subscribing to all wfx instances (and aggregating the events). Browser Connection Limits for SSE: Web browsers typically restrict the number of SSE connections to six per domain. To overcome this limitation, HTTP/2 can be used, allowing up to 100 connections by default, or filter parameters can be utilized to efficiently manage the connections. Response Filters wfx allows server-side response content filtering prior to sending the response to the client so to tailor it to client information needs. For example, if a client isn’t interested in the job’s history, it may request to omit it in wfx responses ― which also saves bandwidth. To this end, clients send a custom HTTP header, X-Response-Filter, with a jq-like expression value. For example, assuming a job with ID 1 exists,\ncurl -s -f http://localhost:8080/api/wfx/v1/jobs/1/status \\ -H \"X-Response-Filter: .state\" returns the current state of the job as a string.\nNote that the (filtered) response might no longer be a valid JSON expression as is the case in this example. It’s the client’s responsibility to handle the filtered response properly ― which it asked for being filtered in the first place.\nHealth Check wfx includes an internal health check service that’s accessible at /health, e.g., via\ncurl http://localhost:8080/health in standard configuration or, alternatively,\nwfxctl health wfx Version The version of wfx running is accessible at /version, e.g., via\ncurl http://localhost:8080/version in standard configuration or, alternatively,\nwfxctl version Deployment To assist a secure deployment, both, the northbound operator/management interface and the southbound client interface, are isolated from each other by being bound to distinct ports so that, e.g., a firewall can be used to steer and restrict access.\nWhile this separation provides a basic level of security, it doesn’t prevent clients interfering with each other: For example, there is no mechanism in place to prevent a client A from updating the jobs of another client B. This is a deliberate design choice following the Unix philosophy of “Make each program do one thing well”. It allows for flexible integration of wfx into existing or new infrastructure. Existing infrastructure most probably has request authentication and authorization measures in place. For new infrastructure, such a feature is likely better provided by specialized components/services supporting the overall deployment security architecture.\nThus, for productive deployments, a deployment along the lines of the following figure is recommended with an API Gateway subsuming the discussed security requirements and performing access steering with regard to, e.g., client access.\n┌──────────────────┐ │ Operator / │ │ Management │ └──────────────────┘ │ │ Request Authentication \u0026 Authorization ▼ ┌──────────────────┐ │ API Gateway │ └──────────────────┘ │ │ Northbound: Management API ▼ ┌──────────────────┐ │ wfx │ └──────────────────┘ ▲ │ Southbound: Client API │ ┌──────────────────┐ │ API Gateway │ └──────────────────┘ ▲ │ Request Authentication \u0026 Authorization │ ┌──────────────────┐ ┌┴─────────────────┐│ │ Client ├┘ └──────────────────┘ Plugins wfx offers a flexible (out-of-tree) plugin mechanism for extending its request processing capabilities. A plugin functions as a subprocess, both initiated and supervised by wfx. Communication between the plugin and wfx is facilitated through the exchange of flatbuffer messages over stdin/stdout, thus permitting plugins to be developed in any programming language.\nDesign Choices Due to the potential use of plugins for authentication, it is critical that all requests are passed through the plugins before further processing and that that no request can slip through without being processed by the plugins. This has led to the following deliberate design choices:\nShould any plugin exit (e.g., due to a crash), wfx is designed to terminate gracefully. While it might be feasible for wfx to attempt restarting the affected plugins, this responsibility is more suitably handled by a dedicated process supervisor like systemd. The shutdown of wfx enables the process supervisor to restart wfx, which, in turn, starts all its plugins again. All plugins are initialized before wfx starts processing any requests. In particular, after the completion of wfx’s startup phase, it’s not possible to add or remove any plugins. Plugins are expected to function properly. Specifically, if a plugin returns an invalid response type or an unexpected response (for example, in response to a request that was never sent to the plugin), wfx will terminate gracefully. This is because such behavior usually indicates a misconfiguration. The overall strategy is to fail fast and early. Using Plugins To use plugins at runtime, wfx must be compiled with the plugin tag (enabled by default) and started with the --mgmt-plugins-dir resp. --client-plugins-dir flag, specifying a directory containing the plugins to be used. This enables the use of different plugin sets for the north- resp. southbound API.\nNote: In a plugin directory, all executable files (including symlinks to executables) are assumed to be plugins. Non-executable files, like configuration files, are excluded. For deterministic behavior, plugins are sorted and executed in lexicographic order based on their filenames during the startup of wfx.\nDeveloping Plugins Communication between wfx and a plugin is achieved by exchanging flatbuffer messages via stdin/stdout. The flatbuffer specification is available in the fbs directory. A plugin can use stderr for logging purposes (stderr is forwarded and prefixed by wfx).\nFor every incoming request, wfx generates a unique number called cookie. The cookie, along with the complete request (e.g., headers and body in the case of HTTP), is written to the plugin’s stdin. The plugin then sends its response, paired with the same cookie, back to wfx by writing to its stdout. This cookie mechanism ensures that wfx can accurately associate responses with their corresponding requests.\nTechnical Note: Cookies are represented as unsigned 64-bit integers, which may lead to wraparound. This means there is a slight possibility that a cookie could be reused for more than one request over the lifespan of a plugin. However, this event occurs only once every 2^64 requests. By the time such a reuse might happen, the original request associated with the cookie would have already timed out.\nNote:\nIt is crucial for the plugin to read data from its stdin descriptor promptly to prevent blocking writes by wfx. The cookie mechanism facilitates (and encourages!) asynchronous processing. The working directory for the plugin process is the same as the working directory of wfx, which is the directory from which wfx was launched. Based on the plugin’s response, wfx can:\nModify the incoming request before it undergoes further processing by wfx in the usual manner. Send a preemptive response back to the client, such as a “permission denied” or “service unavailable” message. Leave the request unchanged. Use Cases Plugins are typically used for:\nEnforcing authentication and authorization for API endpoints. Handling URL rewriting and redirection tasks. Example An example plugin written in Go demonstrates denying access to the /api/wfx/v1/workflows endpoint.\nTelemetry No telemetry or user data is collected or processed by wfx.\nNote that there is an indirect dependency on go.opentelemetry.io/otel via Go OpenAPI by the client runtime (as used by wfxctl). Telemetry is deliberately turned off in wfx. See Add support for tracing via OpenTelemetry for details.\nPerformance / Benchmarking wfx has been designed with performance and horizontal scalability in mind.\nTo regress-test and gauge the performance of wfx in particular scenarios, the wfx-loadtest tool stressing the REST API can be helpful. It’s build by default alongside the other wfx binaries, see Section Building wfx.\nAs an example, the following commands execute a benchmark of the default SQLite persistent storage:\nwfx --log-format json --log-level warn \u0026 wfx-loadtest --log-level warn --duration 60s Note: In the above example, the log format is JSON since pretty-printing is an expensive operation.\nThe benchmark result including statistics is printed to the terminal after 60 seconds and also available in the results directory for further inspection.\n******************************************************************************* Summary ******************************************************************************* Requests [total, rate, throughput] 6000, 100.02, 100.02 Duration [total, attack, wait] 59.987s, 59.986s, 792.516µs Latencies [min, mean, 50, 90, 95, 99, max] 187.363µs, 612.69µs, 602.423µs, 744.281µs, 809.622µs, 1.062ms, 7.242ms Bytes In [total, mean] 4041243, 673.54 Bytes Out [total, mean] 81693, 13.62 Success [ratio] 100.00% Status Codes [code:count] 200:5625 201:375 Error Set: and is to be interpreted as follows: There were 6,000 total requests at a rate of 100.02 requests per second. In terms of latency, the minimum response time was 187.363 microseconds, the mean was 612.69 microseconds, and the 99th percentile was 1.062 milliseconds. All requests were successful with a success ratio of 100%. The status codes indicate that a total of 5,625 status updates were sent with HTTP error code 200 and 375 jobs were created with HTTP error code 201. No errors were reported.\nThe latency over time distribution is illustrated in the following figure: ","categories":"","description":"","excerpt":"API wfx provides two RESTful APIs to interact with it: the northbound …","ref":"/docs/operations/","tags":"","title":"Operations"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/docs/api/","tags":"","title":"REST API"},{"body":"","categories":"","description":"","excerpt":"","ref":"/tags/","tags":"","title":"Tags"},{"body":"This document presents a collection of use cases for employing wfx. Owing to wfx’s versatility, the list provided here is not exhaustive. Should you identify an important use case missing, please feel free to contribute.\nNote: Each use case requires an appropriate client implementation to execute the specific “business logic”. If no such client exists yet, this implies writing custom code.\nSoftware Update Perform over-the-air (OTA) firmware updates using SWUpdate and the Device Artifact Update (DAU) Workflow Family. See also SWUpdate’s Suricatta documentation.\nRemote Access Establish remote terminal (debug) sessions to devices, e.g. for diagnostic purposes. The process involves:\nCreating a custom workflow wfx.workflow.remote.access that encapsulates the steps to initiate a remote terminal session. Generating a new job for the device using this workflow. The job metadata could contain authentication credentials. The client checks wfx periodically for new jobs. Upon finding the job from step 2, it opens its firewall and starts a (secure) service to accept remote terminal connections. Following a pre-set timeout (e.g. configurable in the job metadata), the client will close its firewall and terminate the terminal service. A proof-of-concept demonstrating remote terminal connections via WebSockets (browser-based) is available here.\nConfig Deployment Another common use case involves configuration deployment, akin to the Software Update use case. The goal is to roll out a new configuration to one or multiple client(s), leading to the restart of certain services. Again this requires defining an appropriate workflow and a corresponding client. A proof-of-concept is available here.\n","categories":"","description":"","excerpt":"This document presents a collection of use cases for employing wfx. …","ref":"/docs/use-cases/","tags":"","title":"Use Cases"},{"body":" Learn More Download ","categories":"","description":"","excerpt":" Learn More Download ","ref":"/","tags":"","title":"wfx"},{"body":"A workflow is a finite-state machine that is defined in YAML format and follows specific rules and constraints. More precisely, a workflow consists of\na non-empty unique name, a non-empty finite set of named states, a non-empty finite set of transitions correlating states by their names, and optionally, a set of groups collating states by their names, see the wfx OpenAPI Specification’s Workflow object for reference.\nwfx supports dynamic loading and unloading of workflows at run-time; when a workflow is loaded into wfx, it is validated to ensure that it adheres to these rules and constraints. Upon loading a workflow into wfx it is persistently stored (see Configuration) and is henceforth immutable. However, as long as there is no job referencing it ― including finished ones ― workflows can be unloaded, i.e., deleted from wfx’s persistent storage. After having corrected the unloaded workflow, it can be loaded into wfx again.\nGraph-wise, the set of states defines the nodes of the finite-state machine graph and the set of transitions defines the directed edges among the nodes. In the optional set of groups, disjoint sets of states can be combined for semantic grouping and easier state query selection.\nEach state in states consists of\na unique name, and a description. Each group in groups consists of\na unique name and a non-empty set of states’ names. Each transition in transitions consists of\na starting state name from matching one of the unique state names in states, an ending state name to matching one of the unique state names in states, an eligible attribute denoting the entity that may execute the transition, either CLIENT or WFX, and an optional action attribute that ― depending on the eligible entity ― specifies the transition execution action. Note: Trivial transitions, where the source and destination states are the same (from == to), are implicit in the workflow. These transitions allow the client to report progress within the same state without requiring the transition to be explicitly defined in the workflow (thereby clobbering the workflow).\nThe currently valid actions for WFX-eligible transitions are\nIMMEDIATE: wfx instantly transitions to the transition’s ending state to, and WAIT: external north-bound input, e.g., by an operator or a higher-up hierarchical wfx is required to advance the workflow with WAIT being the default WFX action.\nFor CLIENT-eligible transitions, there are currently no pre-defined actions to assign to in a workflow specification. Instead, the transition execution actions are encoded in the client implementations which are specific to a workflow (family).\nExtending the available actions for WFX-eligible transitions and providing actions for CLIENT-eligible transitions is recorded in the Roadmap.\nBeyond syntactic requirements such as, e.g., a particular set being non-empty or the uniqueness property of names, the following semantic rules and constraints are checked and enforced by wfx upon loading a workflow:\nThere’s exactly one initial state, i.e., there are no incoming transitions to this state with this state’s name in to. There are no unreachable states, i.e., states without an incoming transition. For each state, there can’t be more than one outgoing transition whose action is IMMEDIATE. Transition tuples (from, to, eligible, action) must be unique. There are no cycles in the workflow graph except for trivial cycles, i.e. transitions where from equals to (used for e.g. progress reporting). Each state belongs to at most one group. Tip: wfxctl can be used to validate workflows offline, e.g.\nwfxctl workflow validate workflow/dau/wfx.workflow.dau.direct.yml These definitions are illustrated in more detail in the following exemplary Kanban workflow.\nJobs A job is an instance of a workflow in which wfx and a client progress in lock-step. As a result, a job can only be created if the corresponding workflow already exists. Jobs can be driven by different workflows. This allows wfx to accommodate a variety of use cases and workflows that are tailored to specific needs. For example, firmware and container image updates may have different requirements and constraints that require separate workflows to address them.\nCreating Jobs A new job can be created by sending a JobRequest object (see wfx OpenAPI Specification) to wfx’s northbound REST API. A JobRequest consists of the following:\na non-empty clientId to assign the job to a specific client a non-empty workflow name to select a workflow for the job an optional array of tags that can be used to query the job an optional job definition, which is a freeform JSON object that provides job-specific data to the client. For example, the job definition could contain a download URL for an encrypted firmware artifact that only the specific client can decrypt.\nWhen wfx accepts the JobRequest, a new Job entity is created, and the following fields are populated by wfx:\nid: the globally unique job id state: the unique initial state of the workflow that drives the job stime, mtime: the date and time (ISO8601) when the job was created status.definitionHash: a hash value computed over the definition field, used to detect job definition modifications. Updating Jobs After a job has been created, its definition, status, and tags can be updated using wfx’s REST APIs. If a job definition is updated after a client has already started working on the job, this assumes that the client can handle the changes. To simplify things for client authors, wfx automatically updates the status.definitionHash whenever the definition changes. This provides a mechanism for detecting changes in the definition.\nJob History Whenever a job’s status or definition changes, wfx prepends the current value to the job’s history array. This allows for reviewing job updates later on and diagnosing problems.\nNote: By default, the job history is omitted from all Job responses, primarily due to its diagnostic nature but also for bandwidth reasons. Clients requiring a job’s historical data must explicitly request the specific job and include the history=true parameter in their request.\nDeleting Jobs Jobs can be deleted using the northbound REST API. For example, this can be used to perform maintenance on old jobs. Note that wfx does not perform any housekeeping on its own.\nKanban Example Workflow An exemplary Kanban-inspired workflow YAML specification may be defined as follows:\nname: wfx.workflow.kanban groups: - name: OPEN description: The task is ready for the client(s). states: - NEW - PROGRESS - VALIDATE - name: CLOSED description: The task is in a final state, i.e. it cannot progress any further. states: - DONE - DISCARDED states: - name: BACKLOG description: Task is created - name: NEW description: task is ready to be pulled - name: PROGRESS description: task is being worked on - name: VALIDATE description: task is validated - name: DONE description: task is done according to the definition of done - name: DISCARDED description: task is discarded transitions: - from: BACKLOG to: NEW eligible: WFX action: IMMEDIATE description: | Immediately transition to \"NEW\" upon a task hitting the backlog, conveniently done by wfx \"on behalf of\" the Product Owner. - from: NEW to: PROGRESS eligible: CLIENT description: | A Developer pulls the task or the Product Owner discards it (see below transition), whoever comes first. - from: NEW to: DISCARDED eligible: WFX description: | The Product Owner discards the task or a Developer pulls it (see preceding transition), whoever comes first. - from: PROGRESS to: VALIDATE eligible: CLIENT description: | The Developer has completed the task, it's ready for validation. - from: PROGRESS to: PROGRESS eligible: CLIENT description: | The Developer reports task completion progress percentage. - from: VALIDATE to: DISCARDED eligible: WFX description: | The task result has no customer value. - from: VALIDATE to: DISCARDED eligible: CLIENT description: | The task result cannot be integrated into Production software. - from: VALIDATE to: DONE eligible: CLIENT description: | A Developer has validated the task result as useful. - from: VALIDATE to: DONE eligible: WFX action: WAIT description: | The Product Owner has validated the task result as useful resulting in the following graph representation:\nBACKLOG │ ▼ NEW ───────┐ │ │ ├◀─┐ │ ▼ │ │ PROGRESS ─────┤ │ │ ▼ │ VALIDATE ─────┤ │ │ ▼ ▼ DONE DISCARDED Hands-on: Playing Kanban Assuming the preceding Kanban workflow is saved in the file wfx.workflow.kanban.yml,\nwfxctl workflow create --filter=.transitions wfx.workflow.kanban.yml loads the Kanban workflow into wfx making it available to create jobs driven by it. Note that the the command’s output was made less verbose by using a --filter to only show the transitions.\nHenceforth, a job is identified with a “Task” and a state is identified with a “Lane” in Kanban board parlance. The Product Owner Parker as also owning and managing the Kanban “Board” instruments the northbound wfx management interface while Developers instrument the southbound client interface.\nNow that the Kanban workflow is available, Product Owner Parker creates a new Backlog item ― overriding the Pull principle ― to be taken by the highly specialized Developer Dana:\necho '{ \"title\": \"expose job api\" }' | \\ wfxctl job create --workflow wfx.workflow.kanban \\ --client-id dana \\ --filter='del(.workflow)' - The Task is immediately transitioned to the “NEW” state (Kanban Lane) by wfx on behalf of the Product Owner Parker as the transition’s action is IMMEDIATE. Note that the piped-in JSON document is the Job Definition (see wfx OpenAPI Specification) which is the contract between the operator (Product Owner) creating jobs (Tasks) and the clients (Developers) executing those jobs so that they’re actually able to process the job.\nThen, Developer Dana, knowing the job (Task) Identifier, pulls the task into “PROGRESS”\nwfxctl job update-status \\ --actor=client \\ --id=1 \\ --state=PROGRESS and starts working on it.\nTraditional tools like curl can also be used instead of wfxctl to achieve the same result:\ncurl -X PUT \\ http://localhost:8080/api/wfx/v1/jobs/1/status \\ -H 'Content-Type: application/json' \\ -H 'Accept: application/json' \\ -d '{\"state\":\"PROGRESS\"}' Meanwhile, Developer Dana sporadically reports progress\nwfxctl job update-status \\ --actor=client \\ --id=1 \\ --state=PROGRESS \\ --progress $((RANDOM % 100)) until having finished the task and progressing it into the “VALIDATE” state:\nwfxctl job update-status \\ --actor=client \\ --id=1 \\ --state=VALIDATE Then, Developer Dana may realize that the task result cannot be integrated into the Production software and may put it into “DISCARDED” or Product Owner Parker may come to the conclusion that no customer value is provided by the task result, also putting it into “DISCARDED” ― whoever realizes this first.\nOr, if the task result greatly increases customer value, Developer Dana as being highly experienced may do the validation herself, thereafter putting the task to “DONE”. The Product Owner Parker may come to the same conclusion ― again whoever is faster in realizing the customer benefit.\nAlternatively, if the task in question holds significant potential for increased customer value, Developer Dana, being highly experienced, may undertake the validation process herself and subsequently mark the task as “DONE.” Likewise, Product Owner Parker may arrive at the same conclusion and act accordingly - again, whoever realizes this first may advance the task accordingly.\n","categories":"","description":"","excerpt":"A workflow is a finite-state machine that is defined in YAML format …","ref":"/docs/workflows/","tags":"","title":"Workflows"}]